{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Simulation environment of a food truck.\n",
    "        \"\"\"        \n",
    "        self.demand_values = [100, 200, 300, 400]\n",
    "        self.demand_probabilities = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = 400\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                    'Thu', 'Fri', 'Weekend']\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                            + [(day, inventory) for day in self.days[1:] for inventory in [0, 100, 200, 300]]\n",
    "\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        \"\"\"\n",
    "        Get the next state based on the current statue, action and demand.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple\n",
    "            The state is defined through the day and inventory: state = (day, inventory)\n",
    "        action : int\n",
    "            Number of burger patties bought to refill the inventory\n",
    "        demand : int\n",
    "            Number of burgers requested \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Resulting consequences for the next state\n",
    "        \"\"\"        \n",
    "        day, inventory = state\n",
    "        result = dict()\n",
    "        result['next_day'] = self.days[self.days.index(day) + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_probability(self, state, action):\n",
    "        \"\"\"\n",
    "        Get the transition probability values for the given state and action.\n",
    "        Rate the next state and the according reward with the probability of the event.\n",
    "        The probability is derived from the assumed probability of the demand. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple\n",
    "            The state is defined through the day and inventory: state = (day, inventory)\n",
    "        action : int\n",
    "            Number of burger patties bought to refill the inventory\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            State and reward tuples with corresponding probability\n",
    "        \"\"\"        \n",
    "        next_state_and_reward_probability = dict()\n",
    "        for index, demand in enumerate(self.demand_values):\n",
    "            result = self.get_next_state_reward(state, action, demand)\n",
    "            next_state = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            probability = self.demand_probabilities[index]\n",
    "            if (next_state, reward) not in next_state_and_reward_probability:\n",
    "                next_state_and_reward_probability[next_state, reward] = probability\n",
    "            else:\n",
    "                next_state_and_reward_probability[next_state, reward] += probability\n",
    "        return next_state_and_reward_probability\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        day, _ = state\n",
    "        if day == 'Weekend':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        return (self.day, self.inventory)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Simulate the environment for one time step for the current state and action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Action at the current state. Number of burger patties to buy for refilling the inventory.\n",
    "        \"\"\"        \n",
    "        demand = np.random.choice(a=self.demand_values, p=self.demand_probabilities)\n",
    "        result = self.get_next_state_reward(\n",
    "            state=(self.day, self.inventory),\n",
    "            demand=demand,\n",
    "            action=action)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "        return state, reward, done, info\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_policy(states):\n",
    "    \"\"\"\n",
    "    Defining a base policy. States are mapped to actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    states : list\n",
    "        List of environment states\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The resulting policy. Keys are states while the values are itself dictionaries composed \n",
    "        of action probability pairs.\n",
    "    \"\"\"    \n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_probability_pairs = dict()\n",
    "        day, inventory = state\n",
    "        if inventory >= 300:\n",
    "            action_probability_pairs[0] = 1.0\n",
    "        else:\n",
    "            action_probability_pairs[400 - inventory] = 0.5\n",
    "            action_probability_pairs[300 - inventory] = 0.5\n",
    "        policy[state] = action_probability_pairs\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_update(env, value, state, action_probability_pairs, gamma):\n",
    "    \"\"\"\n",
    "    Update the expected value.\n",
    "    For a given current state, the transition probabilities are calculated. To this end,\n",
    "    the probabilities for the next state with the corresponding reward are available. From this\n",
    "    events, the expected value is calculated. The expected value is the summed over all events. \n",
    "    \n",
    "    Converting the Bellman equation into an update rule.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : _type_\n",
    "        _description_\n",
    "    value : _type_\n",
    "        _description_\n",
    "    state : _type_\n",
    "        _description_\n",
    "    action_probability_pairs : _type_\n",
    "        _description_\n",
    "    gamma : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"    \n",
    "    expected_value = 0\n",
    "    for action in action_probability_pairs:\n",
    "        probability_next_state_and_reward = env.get_transition_probability(state, action)\n",
    "        for next_state, reward in probability_next_state_and_reward:\n",
    "            expected_value += action_probability_pairs[action] \\\n",
    "                            * probability_next_state_and_reward[next_state, reward] \\\n",
    "                            * (reward + gamma * value[next_state])\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iter=100,\n",
    "                      value=None, eps=0.1, gamma=1):\n",
    "    \"\"\"\n",
    "    Policy evaluation executes expected updates for all states until the state value converges or \n",
    "    it reaches the maximum number of iterations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : class object\n",
    "        simulation environment\n",
    "    policy : dict\n",
    "        A policy that maps states to actions. Keys are states while the values are itself dictionaries composed \n",
    "        of action probability pairs.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations if algorithm does not converge earlier, by default 100\n",
    "    value : _type_, optional\n",
    "        Expected discounted cumulative reward starting in state s and following the policy, by default None\n",
    "    eps : float, optional\n",
    "        Acceptance criteria for the convergence of the algorithm, by default 0.1\n",
    "    gamma : int, optional\n",
    "        Discount factor, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Converged expected discounted cumulative rewards\n",
    "    \"\"\"    \n",
    "    if not value:\n",
    "        value = {state: 0 for state in env.state_space}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for state in value:\n",
    "            if not env.is_terminal(state):\n",
    "                value_old = value[state]\n",
    "                action_probability_pairs = policy[state]\n",
    "                value[state] = expected_update(env, value, state, action_probability_pairs, gamma)\n",
    "                max_delta = max(max_delta, abs(value[state]- value_old))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(f\"Converged in {k} iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(f\"Terminating after {k} iterations.\")\n",
    "            break\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    action_probability = policy[state]\n",
    "    action = np.random.choice(\n",
    "        a=list(action_probability.keys()),\n",
    "        p=list(action_probability.values())\n",
    "        )\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(policy, num_episodes):\n",
    "    np.random.seed(0)\n",
    "    foodTruck = FoodTruck()\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = foodTruck.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, policy=policy)\n",
    "            state, reward, done, info = foodTruck.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    print(f\"Expected weekly profit: {np.mean(rewards)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodTruck = FoodTruck()\n",
    "policy = base_policy(foodTruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Expected weekly profit: 2510.3852187500006\n"
     ]
    }
   ],
   "source": [
    "value = policy_evaluation(env=foodTruck, policy=policy)\n",
    "print(f\"Expected weekly profit: {value['Mon', 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected weekly profit: 2575.0\n"
     ]
    }
   ],
   "source": [
    "simulate_policy(policy=policy, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo estimation of state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_return(returns, trajectory, gamma):\n",
    "    \"\"\"\n",
    "    Calculate the returns from the first visit on for each state that appears in the trajectory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : dict\n",
    "        Returns corresponding to trajectory. Keys are states and values are list of returns calculated from some other trajectory.\n",
    "    trajectory : list\n",
    "        List of state, action, reward tuples\n",
    "    gamma : float\n",
    "        Discount factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated returns\n",
    "    \"\"\"    \n",
    "    G = 0 # cumulated discounted reward\n",
    "    T = len(trajectory) - 1\n",
    "    for t, state_action_reward in enumerate(reversed(trajectory)):\n",
    "        state, action, reward = state_action_reward\n",
    "        G = reward + gamma * G\n",
    "        first_visit = True\n",
    "        for j in range(T - t):\n",
    "            if state == trajectory[j][0]:\n",
    "                first_visit = False\n",
    "            if first_visit:\n",
    "                if state in returns:\n",
    "                    returns[state].append(G)\n",
    "                else:\n",
    "                    returns[state] = [G]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    state_action_reward = [state]\n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state_action_reward.append(action)\n",
    "        state_action_reward.append(reward)\n",
    "        trajectory.append(state_action_reward)\n",
    "        state_action_reward = [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, num_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    values = {}\n",
    "    for _ in range(num_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns, trajectory, gamma)\n",
    "    for state in env.state_space:\n",
    "        if state in returns:\n",
    "            values[state] = np.round(np.mean(returns[state]), 1)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, num_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    values = {}\n",
    "    for _ in range(num_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns, trajectory, gamma)\n",
    "    for state in env.state_space:\n",
    "        if state in returns:\n",
    "            values[state] = np.round(np.mean(returns[state]), 1)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodTruck = FoodTruck()\n",
    "policy = base_policy(foodTruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mon', 0),\n",
       " ('Tue', 0),\n",
       " ('Tue', 100),\n",
       " ('Tue', 200),\n",
       " ('Tue', 300),\n",
       " ('Wed', 0),\n",
       " ('Wed', 100),\n",
       " ('Wed', 200),\n",
       " ('Wed', 300),\n",
       " ('Thu', 0),\n",
       " ('Thu', 100),\n",
       " ('Thu', 200),\n",
       " ('Thu', 300),\n",
       " ('Fri', 0),\n",
       " ('Fri', 100),\n",
       " ('Fri', 200),\n",
       " ('Fri', 300),\n",
       " ('Weekend', 0),\n",
       " ('Weekend', 100),\n",
       " ('Weekend', 200),\n",
       " ('Weekend', 300)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodTruck.state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): {400: 0.5, 300: 0.5},\n",
       " ('Tue', 0): {400: 0.5, 300: 0.5},\n",
       " ('Tue', 100): {300: 0.5, 200: 0.5},\n",
       " ('Tue', 200): {200: 0.5, 100: 0.5},\n",
       " ('Tue', 300): {0: 1.0},\n",
       " ('Wed', 0): {400: 0.5, 300: 0.5},\n",
       " ('Wed', 100): {300: 0.5, 200: 0.5},\n",
       " ('Wed', 200): {200: 0.5, 100: 0.5},\n",
       " ('Wed', 300): {0: 1.0},\n",
       " ('Thu', 0): {400: 0.5, 300: 0.5},\n",
       " ('Thu', 100): {300: 0.5, 200: 0.5},\n",
       " ('Thu', 200): {200: 0.5, 100: 0.5},\n",
       " ('Thu', 300): {0: 1.0},\n",
       " ('Fri', 0): {400: 0.5, 300: 0.5},\n",
       " ('Fri', 100): {300: 0.5, 200: 0.5},\n",
       " ('Fri', 200): {200: 0.5, 100: 0.5},\n",
       " ('Fri', 300): {0: 1.0},\n",
       " ('Weekend', 0): {400: 0.5, 300: 0.5},\n",
       " ('Weekend', 100): {300: 0.5, 200: 0.5},\n",
       " ('Weekend', 200): {200: 0.5, 100: 0.5},\n",
       " ('Weekend', 300): {0: 1.0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_values = first_visit_mc(env=foodTruck, policy=policy, gamma=1.0, num_trajectories=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Tue', 0): 1902.2,\n",
       " ('Tue', 100): 2335.6,\n",
       " ('Tue', 200): 2715.9,\n",
       " ('Tue', 300): 3086.8,\n",
       " ('Wed', 0): 1341.1,\n",
       " ('Wed', 100): 1729.8,\n",
       " ('Wed', 200): 2092.1,\n",
       " ('Wed', 300): 2389.6,\n",
       " ('Thu', 0): 634.4,\n",
       " ('Thu', 100): 1043.6,\n",
       " ('Thu', 200): 1557.2,\n",
       " ('Thu', 300): 1867.5,\n",
       " ('Fri', 0): 119.8,\n",
       " ('Fri', 100): 384.2,\n",
       " ('Fri', 200): 842.6,\n",
       " ('Fri', 300): 1444.7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n"
     ]
    }
   ],
   "source": [
    "true_values = policy_evaluation(env=foodTruck, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): 2510.3852187500006,\n",
       " ('Tue', 0): 1897.4318750000002,\n",
       " ('Tue', 100): 2297.431875,\n",
       " ('Tue', 200): 2697.431875,\n",
       " ('Tue', 300): 3083.7875000000004,\n",
       " ('Wed', 0): 1283.7875000000001,\n",
       " ('Wed', 100): 1683.7875000000004,\n",
       " ('Wed', 200): 2083.7875,\n",
       " ('Wed', 300): 2474.75,\n",
       " ('Thu', 0): 674.75,\n",
       " ('Thu', 100): 1074.75,\n",
       " ('Thu', 200): 1474.75,\n",
       " ('Thu', 300): 1835.0,\n",
       " ('Fri', 0): 35.00000000000003,\n",
       " ('Fri', 100): 435.0,\n",
       " ('Fri', 200): 835.0,\n",
       " ('Fri', 300): 1400.0,\n",
       " ('Weekend', 0): 0,\n",
       " ('Weekend', 100): 0,\n",
       " ('Weekend', 200): 0,\n",
       " ('Weekend', 300): 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps_greedy(actions, eps, best_action):\n",
    "    probabilities_actions = {}\n",
    "    num_actions = len(actions)\n",
    "    for action in actions:\n",
    "        if action == best_action:\n",
    "            probabilities_actions[action] = 1 - eps + eps / num_actions\n",
    "        else:\n",
    "            probabilities_actions[action] = eps / num_actions\n",
    "    return probabilities_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On policy Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that generates a random policy, where all actions are equally likely to be taken\n",
    "def get_random_policy(states, actions):\n",
    "    policy = {}\n",
    "    num_actions = len(actions)\n",
    "    for state in states:\n",
    "        policy[state] = {action: 1 / num_actions for action in actions}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on policy first-visit MC control algorithm\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc(env, num_iterations, eps, gamma):\n",
    "    np.random.seed(0)\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    policy = get_random_policy(states, actions)\n",
    "    Q = {state: {action: 0 for action in actions} for state in states}\n",
    "    Q_n = {state: {action: 0 for action in actions} for state in states}\n",
    "    for iteration in range(num_iterations):\n",
    "        if iteration % 10000 == 0:\n",
    "            print(f\"Iteration: {iteration}\")\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        G = 0\n",
    "        T = len(trajectory) - 1\n",
    "        for t, state_action_reward in enumerate(reversed(trajectory)):\n",
    "            state, action, reward = state_action_reward\n",
    "            G = reward + gamma * G\n",
    "            first_visit = True\n",
    "            for j in range(T - t):\n",
    "                state_j = trajectory[j][0]\n",
    "                action_j = trajectory[j][1]\n",
    "                if (state, action) == (state_j, action_j):\n",
    "                    first_visit = False\n",
    "                if first_visit:\n",
    "                    Q[state][action] = Q_n[state][action] * Q_n[state][action] + G\n",
    "                    Q_n[state][action] += 1\n",
    "                    Q[state][action] /= Q_n[state][action]\n",
    "                    best_action = max(Q[state].items(), key=operator.itemgetter(1))[0]\n",
    "                    policy[state] = get_eps_greedy(actions, eps, best_action)\n",
    "    return policy, Q, Q_n\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10000\n",
      "Iteration: 20000\n",
      "Iteration: 30000\n",
      "Iteration: 40000\n",
      "Iteration: 50000\n",
      "Iteration: 60000\n",
      "Iteration: 70000\n",
      "Iteration: 80000\n",
      "Iteration: 90000\n",
      "Iteration: 100000\n",
      "Iteration: 110000\n",
      "Iteration: 120000\n",
      "Iteration: 130000\n",
      "Iteration: 140000\n",
      "Iteration: 150000\n",
      "Iteration: 160000\n",
      "Iteration: 170000\n",
      "Iteration: 180000\n",
      "Iteration: 190000\n",
      "Iteration: 200000\n",
      "Iteration: 210000\n",
      "Iteration: 220000\n",
      "Iteration: 230000\n",
      "Iteration: 240000\n",
      "Iteration: 250000\n",
      "Iteration: 260000\n",
      "Iteration: 270000\n",
      "Iteration: 280000\n",
      "Iteration: 290000\n"
     ]
    }
   ],
   "source": [
    "# use the policy iteration\n",
    "policy, Q, Q_n = on_policy_first_visit_mc(foodTruck, 300000, 0.05, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Tue', 0): {0: 0.01, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.96},\n",
       " ('Tue', 100): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Tue', 200): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Tue', 300): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 0): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 100): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 200): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 300): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 0): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 100): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 200): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 300): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 0): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 100): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 200): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 300): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Weekend', 0): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 100): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 200): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 300): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooddTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.demand_values = [100, 200, 300, 400]\n",
    "        self.demand_probabilities = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = 400\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                    'Thu', 'Fri', 'Weekend']\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                            + [(day, inventory) for day in self.days[1:] for inventory in [0, 100, 200, 300]]\n",
    "\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = dict()\n",
    "        # next day\n",
    "        result['next_day'] = self.days[self.days.index(day) + 1]\n",
    "        # starting inventory\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        # cost for action\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        # sales\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        # revenue\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        # next inventory\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        # reward\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_probability(self, state, action):\n",
    "        next_state_and_reward_probability = dict()\n",
    "        for index, demand in enumerate(self.demand_values):\n",
    "            result = self.get_next_state_reward(state, action, demand)\n",
    "            next_state = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            probability = self.demand_probabilities[index]\n",
    "            if (next_state, reward) not in next_state_and_reward_probability:\n",
    "                next_state_and_reward_probability[next_state, reward] = probability\n",
    "            else:\n",
    "                next_state_and_reward_probability[next_state, reward] += probability\n",
    "        return next_state_and_reward_probability\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        if day == 'Weekend':\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_probability_pairs = dict()\n",
    "        day, inventory = state\n",
    "        if inventory >= 300:\n",
    "            action_probability_pairs[0] = 1.0\n",
    "        else:\n",
    "            action_probability_pairs[400 - inventory] = 0.5\n",
    "            action_probability_pairs[300 - inventory] = 0.5\n",
    "        policy[state] = action_probability_pairs\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_update(env, value, state, action_probability_pairs, gamma):\n",
    "    expected_value = 0\n",
    "    for action in action_probability_pairs:\n",
    "        probability_next_state_and_reward = env.get_transition_probability(state, action)\n",
    "        for next_state, reward in probability_next_state_and_reward:\n",
    "            expected_value += action_probability_pairs[action] \\\n",
    "                            * probability_next_state_and_reward[next_state, reward] \\\n",
    "                            * (reward + gamma * value[next_state])\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iter=100,\n",
    "                      value=None, eps=0.1, gamma=1):\n",
    "    if not value:\n",
    "        value = {state: 0 for state in env.state_space}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for state in value:\n",
    "            if not env.is_terminal(state):\n",
    "                value_old = value[state]\n",
    "                action_probability_pairs = policy[state]\n",
    "                value[state] = expected_update(env, value, state, action_probability_pairs, gamma)\n",
    "                max_delta = max(max_delta, abs(value[state]- value_old))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(f\"Converged in {k} iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(f\"Terminating after {k} iterations.\")\n",
    "            break\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodTruck = FooddTruck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next_day': 'Tue',\n",
       " 'starting_inventory': 300,\n",
       " 'cost': 1200,\n",
       " 'sales': 300,\n",
       " 'revenue': 2100,\n",
       " 'next_inventory': 0,\n",
       " 'reward': 900}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodTruck.get_next_state_reward(state=('Mon', 0), action=300, demand=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Wed', 200), 300): 0.3,\n",
       " (('Wed', 100), 1000): 0.4,\n",
       " (('Wed', 0), 1700): 0.30000000000000004}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodTruck.get_transition_probability(('Tue', 200), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = base_policy(foodTruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Expected weekly profit: 2510.3852187500006\n"
     ]
    }
   ],
   "source": [
    "value = policy_evaluation(env=foodTruck, policy=policy)\n",
    "print(f\"Expected weekly profit: {value['Mon', 0]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
